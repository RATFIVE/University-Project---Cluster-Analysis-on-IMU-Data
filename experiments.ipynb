{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Exam - Revolutionizing Personal Fitness with Personalized Features Using the New Apple Watch Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License: *This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license. Provided by: Billur Barshan Kerem Altun DOI: 10.24432/C5C59F*\n",
    "\n",
    "*This allows for the sharing and adaptation of the datasets for any purpose, provided that the appropriate credit is given.*\n",
    "\n",
    "*URL: https://archive.ics.uci.edu/dataset/256/daily+and+sports+activities*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story <a name=\"section-1\"></a>\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "My Name is Marco and I am a lead Data Scientist in the Data Science Hub at Apple Inc. For our next experiment we want to find out whether we can find distinct clusters in our Apple Watch users. Equipped with advanced accelerometers, gyroscopes, and magnetometers, the new Apple Watch Series can capture rich data on users' physical activities. Leveraging this data, we are implementing clustering analysis to better understand user exercises, habits and preferences.\n",
    "\n",
    "Our mission at Apple is to empower individuals to lead healthier lives through innovative technology. Our next goal is to introduce individualized training plans with the new Apple Watch Series. To achieve this goal we have provided this experiment where we want find groups aka clusters within our users.  \n",
    "\n",
    "**Experiment Plan:**\n",
    "\n",
    "For this experiment, we recruited eight subjects, comprising 4 females and 4 males, to perform 19 different exercises for 5 minutes each. Each subject wore our new Apple Watch sensors on their torso, arms, and legs. For this experiment, we are focusing just on the sensors located on the right and left arms, where the Apple Watch is typically worn.  The other sensor locations on e.g. torso are necessary for future product development. Our objective is to identify clusters within the raw sensor data without complex feature engineering or sensor fusion.\n",
    "\n",
    "This experiment involves using cutting-edge clustering algorithms to group users into distinct clusters based on their activity patterns and intensity levels. This will enable us to provide customized fitness recommendations and optimize the Apple Watch experience for each user. Our primary goal in this experiment is to determine whether it’s feasible to extract meaningful information just from the raw sensor data on just the arm location.\n",
    "\n",
    "**Expected Value:**\n",
    "\n",
    "Imagine a future where every Apple Watch user receives personalized fitness insights directly from their device. By leveraging clustering analysis, we aim to enhance user engagement, improve workout effectiveness, and provide actionable insights to help users achieve their fitness goals. The implementation of clustering analysis on the Apple Watch has the potential to revolutionize personal fitness, delivering personalized recommendations and optimizing workout programs based on the user activity level. From this we expect an increase user satisfaction and loyalty to the Apple ecosystem.\n",
    "\n",
    "**Sensor Types:**\n",
    "\n",
    "The sensors we’re analyzing include the Accelerometer, Gyroscope, and Magnetometer. These sensors are key components of the new Apple Watch Series.\n",
    "\n",
    "- Accelerometer: Measures acceleration forces and detects changes in velocity or orientation relative to free fall.\n",
    "\n",
    "- Gyroscope: Measures and maintains orientation and angular velocity of an object.\n",
    "\n",
    "- Magnetometer: Measures the strength and direction of magnetic fields around it.\n",
    "\n",
    "The Sensor units are calibrated to acquire data at a 25 Hz sampling frequency, meaning data is collected every 0.04 seconds. Each 5-minute activity signals are divided into 5-second segments, resulting in 480 signal segments for each activity.\n",
    "\n",
    "**Initial Experiment:**\n",
    "\n",
    "Our initial focus is on identifying patterns directly from the raw sensor data, without any additional calculations. By doing so, we aim to understand whether meaningful clusters emerge.\n",
    "\n",
    "**Feature Engineering Techniques which can be applied in future work:**\n",
    "\n",
    "- Kalman filters\n",
    "- Madgwick Orientation Filter\n",
    "- Feature Engineering (e.g., statistical measurements such as skewness, kurtosis, energy, and signal magnitude area)\n",
    "- Fourier transformation\n",
    "\n",
    "**Target Locations & Target Activity:**\n",
    "\n",
    "We concentrate on the sensor data from the left and right arms, the precise locations where individuals typically wear their Apple Watches. This approach allows us to isolate the Apple Watch specific patterns.\n",
    "\n",
    "To simplify this first experiment we are considering just the running activity (a12) for the cluster analysis. (considering the whole data, lead to high runtime and to Kernel failure)\n",
    "\n",
    "**Future Directions:**\n",
    "\n",
    "While our current investigation focuses on the arms, we acknowledge the importance of other sensor locations. As future technologies evolve, we'll explore additional sensor placements for more comprehensive experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensor data is stored in subdirectories. In the directory *data* are 19 directories with the activities a01, a02 ... a19. In each of those directories are eight subdirectories for each of the subjects p1, p2 ... p8. In each of those directories are the sensor files s01.txt, s02.txt ... s60.txt.\n",
    "\n",
    "**Data structure:**\n",
    "- *data*\n",
    "  - *a01*\n",
    "    - *p1*\n",
    "    - *p2*\n",
    "      - *s01.txt*\n",
    "      - *s02.txt*\n",
    "      - *...*\n",
    "  - *a02*\n",
    "    - *p1*\n",
    "    - *p2*\n",
    "      - *s01.txt*\n",
    "      - *s02.txt*\n",
    "      - *...*\n",
    "    - *...* \n",
    "    - \n",
    "**Explaining the features:**\n",
    "`subject`: All eight subjects p1, p2, ... p7, p8\n",
    "\n",
    "`activity`: The 19 activities are: \n",
    "- sitting (A1), \n",
    "- standing (A2), \n",
    "- lying on back and on right side (A3 and A4), \n",
    "- ascending and descending stairs (A5 and A6), \n",
    "- standing in an elevator still (A7) \n",
    "- and moving around in an elevator (A8), \n",
    "- walking in a parking lot (A9), \n",
    "- walking on a treadmill with a speed of 4 km/h (in flat and 15 deg inclined positions) (A1\n",
    "- 0 and A11),\n",
    "- running on a treadmill with a speed of 8 km/h (A12), \n",
    "- exercising on a stepper (A13), \n",
    "- exercising on a cross trainer (A14), \n",
    "- cycling on an exercise bike in horizontal and vertical positions (A15 and A16),\n",
    "- rowing (A17), \n",
    "- jumping (A18), \n",
    "- and playing basketball (A19)\n",
    "\n",
    "`timestamp`: timestamp of the data which was calculated during the concatenation of the data. \n",
    "\n",
    "`T_xacc ... LL_zmag`: Columns 4-49 correspond to: \n",
    "- T_xacc,  T_yacc,  T_zacc,  T_xgyro, ...,  T_ymag,  T_zmag,\n",
    "\n",
    "- RA_xacc, RA_yacc, RA_zacc, RA_xgyro, ..., RA_ymag, RA_zmag,\n",
    "\n",
    "- LA_xacc, LA_yacc, LA_zacc, LA_xgyro, ..., LA_ymag, LA_zmag,\n",
    "\n",
    "- RL_xacc, RL_yacc, RL_zacc, RL_xgyro, ..., RL_ymag, RL_zmag,\n",
    "\n",
    "- LL_xacc, LL_yacc, LL_zacc, LL_xgyro, ..., LL_ymag, LL_zmag\n",
    "\n",
    "5 units on torso (T), right arm (RA), left arm (LA), right leg (RL), left leg (LL) and 9 sensors on each unit (x,y,z accelerometers, x,y,z gyroscopes, x,y,z magnetometers)\n",
    "\n",
    "\n",
    "Therefore:\n",
    "\n",
    "columns  4-12  correspond to the sensors in unit 1 (T), \n",
    "\n",
    "columns 13-22 correspond to the sensors in unit 2 (RA), \n",
    "\n",
    "columns 23-32 correspond to the sensors in unit 3 (LA), \n",
    "\n",
    "columns 33-42 correspond to the sensors in unit 4 (RL), \n",
    "\n",
    "columns 43-48 correspond to the sensors in unit 5 (LL).\n",
    "\n",
    "\n",
    "First we have to iterate over all directories and concatenate all of those *.txt* files into one data_all.csv file. This is done with <code>pd.concat()</code>. This process is done in a separate jupyter file ***collect_data.ipynb*** and stored the data in **data_all.csv**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the necessary libraries for this experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import randint\n",
    "from kneed import KneeLocator\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from pca import pca\n",
    "import warnings\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Ignore user warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning, message='RunTimeWarning')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None) # Display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is stored in the *data_all.csv* file, we utilize the `pd.read_csv('data_all.csv')` function to import it into a dataframe named `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data_all.csv\n",
    "df = pd.read_csv('data_all.csv')\n",
    "\n",
    "# Sort data by subject, activity, and timestamp\n",
    "df.sort_values(by=['subject', 'activity', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - IDA\n",
    "\n",
    "Conducting an initial data analysis we present some distributions and statistical properties that inform the reader about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print('Data: \\n')\n",
    "display(df.head(3))\n",
    "print('Data shape: ', df.shape)\n",
    "print('Data Info:', df.info())\n",
    "print('Data NaN values:', df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Observation:</b>\n",
    "\n",
    "<li> The dataset contains 1,144,000 rows and 48 columns. </li> \n",
    "<li> The first column contains the subjects engaged in various activities. [p1, p2, ..., p7, p8] </li>\n",
    "<li> The second column contains the activities that are completed by each subject. [[a01, a02, ..., a18, a19]]</li>\n",
    "<li> The third column is the timestamp of the data, which is calculated 0.04Hz * index.</li>\n",
    "<li> Columns four through forty-eight contain sensor data, segmented into the x, y, and z axes for each sensor.</li> \n",
    "<li> There are no NaN values present in this dataset. </li>\n",
    "<li> Both the subject and activity columns exclusively contain categorical string values, they have to be preprocessed for the clustering analysis. </li>\n",
    "<li> The sensor data is appropriately formatted as floats.</li>\n",
    "<li>There are no missing values in the data </li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the activity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.countplot(x = 'activity', data = df, hue='activity', palette='Spectral')\n",
    "plt.title('Number of samples by activity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Observation:</b>\n",
    "\n",
    "All the data from activities are equal and containing 60.000 entries per activity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 6))\n",
    "sns.countplot(x = 'subject', hue = 'activity', data = df, palette='Spectral', gap=0.1)\n",
    "plt.title('Activities by Subject')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Observation:</b>\n",
    "\n",
    "All the subjects are equal and have exactly the same amount of entries for each activity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to observe the distribution of the data. Also to see how many outliers are in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 – EDA and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To perform the cluster analysis we first bring the dataset into the form that you need for the experiment.\n",
    "\n",
    "Our focus lies just on the right and left arm data. Consequently, we proceed to create separate dataframes for both arms, denoted as `df_RA_LA`, as well as individual dataframes for the right arm (`df_RA`) and the left arm (`df_LA`). \n",
    "\n",
    "Because in this experiment we want to detect clusters in the running activity, we just consider data from the activity: \n",
    "- running on a treadmill with a speed of 8 km/h (A12),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only the running activity 'a12' \n",
    "df = df.loc[df['activity'].isin(['a12'])]\n",
    "\n",
    "# Dataframes for both right and left arms\n",
    "df_RA_LA = df[['subject','activity', 'timestamp', 'RA_xacc', 'RA_yacc', 'RA_zacc', \n",
    "                    'RA_xgyro', 'RA_ygyro', 'RA_zgyro', \n",
    "                    'RA_xmag', 'RA_ymag', 'RA_zmag',\n",
    "                    'LA_xacc', 'LA_yacc', 'LA_zacc', \n",
    "                    'LA_xgyro', 'LA_ygyro', 'LA_zgyro', \n",
    "                    'LA_xmag', 'LA_ymag', 'LA_zmag']]\n",
    "\n",
    "# Dataframe for just the right arm\n",
    "df_RA = df[['subject','activity', 'timestamp', 'RA_xacc', 'RA_yacc', 'RA_zacc',\n",
    "                          'RA_xgyro', 'RA_ygyro', 'RA_zgyro',\n",
    "                          'RA_xmag', 'RA_ymag', 'RA_zmag']]\n",
    "\n",
    "# Dataframe for just the left arm\n",
    "df_LA = df[['subject','activity','timestamp', 'LA_xacc', 'LA_yacc', 'LA_zacc',\n",
    "                         'LA_xgyro', 'LA_ygyro', 'LA_zgyro',\n",
    "                         'LA_xmag', 'LA_ymag', 'LA_zmag']]\n",
    "\n",
    "print(f'activity a12 (running) data shape: {df_RA_LA.shape}')\n",
    "display(df_RA_LA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Observation:</b>\n",
    "\n",
    "`df_RA_LA` consists now just the running activity a12 and the sensor data of the right arm and left arm location.\n",
    "\n",
    "The same has been done for just the right arm `df_RA` and for the left arm `df_LA`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "In this chapter we are doing exploratory data analysis to get insights about the structure and nature of the data. By visualizing the data we try to identify patterns, trends and anomalies within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we examine descriptive statistics of the sensor data. We exclude the timestamp column as it does not furnish essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_RA.drop('timestamp', axis=1).describe().round(2))\n",
    "display(df_LA.drop('timestamp', axis=1).describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Observation:</b>\n",
    "\n",
    "The upper tables clearly indicates that the accelerometers axis on both arms have a higher standard deviations compared to the gyroscopes and magnetometers. This means that many values in the accelerometers are spread out more widely from its average, while for the gyroscopes and magnetometers, many values fall close to the average value. \n",
    "\n",
    "Additionally, it is apparent that there are numerous outliers in both arms when comparing the minimum and maximum values with the median and the average.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine some statistical diagrams which give us more understanding about the patterns in the running activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Boxplot for each sensor type\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Data comparison between sensor types for left arm and right arm for activity 12', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Set the distance between subplots\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "\n",
    "# Accelerometer Data (Right Arm)\n",
    "sns.violinplot(data=df_RA_LA[['RA_xacc', 'RA_yacc', 'RA_zacc']], ax=ax[0, 0], inner='quart')\n",
    "ax[0, 0].set_title('Accelerometer Data (Right Arm)')\n",
    "\n",
    "# Gyroscope Data (Right Arm)\n",
    "sns.violinplot(data=df_RA_LA[['RA_xgyro', 'RA_ygyro', 'RA_zgyro']], ax=ax[0, 1], inner='quart')\n",
    "ax[0, 1].set_title('Gyroscope Data (Right Arm)')\n",
    "\n",
    "# Magnometer Data (Right Arm)\n",
    "sns.violinplot(data=df_RA_LA[['RA_xmag','RA_ymag', 'RA_zmag']], ax=ax[0, 2], inner='quart')\n",
    "ax[0, 2].set_title('Magnometer Data (Right Arm)')\n",
    "\n",
    "# Accelerometer Data (Left Arm)\n",
    "sns.violinplot(data=df_RA_LA[['LA_xacc', 'LA_yacc', 'LA_zacc']], ax=ax[1, 0], inner='quart')\n",
    "ax[1, 0].set_title('Accelerometer Data (Left Arm)')\n",
    "\n",
    "# Gyroscope Data (Left Arm)\n",
    "sns.violinplot(data=df_RA_LA[['LA_xgyro', 'LA_ygyro', 'LA_zgyro']], ax=ax[1, 1], inner='quart')\n",
    "ax[1, 1].set_title('Gyroscope Data (Left Arm)')\n",
    "\n",
    "# Magnometer Data (Left Arm)\n",
    "sns.violinplot(data=df_RA_LA[['RA_xmag','RA_ymag', 'RA_zmag']], ax=ax[1, 2], inner='quart')\n",
    "ax[1, 2].set_title('Magnometer Data (Left Arm)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Observation:</h3>\n",
    "\n",
    "<b>Accelerometer:</b>\n",
    "The length of the accelerometer violin plot suggests the presence of numerous outliers in the data. Observing the shape of the violin plot, we identify variations in density, indicating uneven distribution across different sections. Furthermore, the presence of multiple peaks in the plot hints at the existence of potential subgroups within the dataset.\n",
    "\n",
    "<b>Gyroscope:</b>\n",
    "The width of the gyroscope violin plot suggests a Gaussian distribution of the data in the x-axis. However, the length of the violin plot highlights significant outliers within the dataset. The densest section of the data clusters around zero. No observeible subgroups are apparent when examining the shape of the violin plot.\n",
    "\n",
    "<b>Magnetometer:</b>\n",
    "The varying lengths of the magnetometer violin plots suggest the presence of higher outliers in specific directions. The shape of the violin plot reveals uneven distribution across the dataset. Additionally, the presence of multiple peaks in the violin plot indicates the existence of distinct subgroups within the data.\n",
    "\n",
    "<b>Comparing left and right arm:</b>\n",
    "Upon comparing the violin plots of the left and right arms, it becomes apparent that they exhibit highly similar distributions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we want to observe how many outliers are contained in the right and left arm dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate outliers in the DataFrame using the IQR method\n",
    "    Source: https://articles.outlier.org/calculate-outlier-formula\n",
    "    '''\n",
    "\n",
    "    df = df.drop(columns=['subject', 'activity', 'timestamp'])\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))   \n",
    "    return outliers\n",
    "\n",
    "# Calculate outliers for each arm separately and combined\n",
    "outliers_RA = calculate_outliers(df_RA)\n",
    "outliers_LA = calculate_outliers(df_LA)\n",
    "outliers_RA_LA = calculate_outliers(df_RA_LA)\n",
    "\n",
    "# Create DataFrame to store outlier counts\n",
    "df_outliers = pd.DataFrame({'Outliers Right Arm': outliers_RA.sum(),\n",
    "                            'Outliers Left Arm': outliers_LA.sum()})\n",
    "df_outliers['Sensor axis'] = df_outliers.index\n",
    "df_outliers = df_outliers.reset_index(drop=True)\n",
    "\n",
    "# Plot outliers for left and right arm data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "fig.suptitle('Outliers in left and right arm data', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Bar plot\n",
    "ax = sns.barplot(data=df_outliers, x='Sensor axis', y='Outliers Right Arm', color='blue', alpha=0.5, ax=ax)\n",
    "ax = sns.barplot(data=df_outliers, x='Sensor axis', y='Outliers Left Arm', color='red', alpha=0.5, ax=ax)\n",
    "\n",
    "plt.ylabel('Number of outliers')\n",
    "plt.xlabel('Sensor axis')\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels\n",
    "plt.subplots_adjust(hspace=1)  # Set distance between subplots\n",
    "plt.show()\n",
    "\n",
    "# Print outlier\n",
    "print('Data shape:', df_RA_LA.shape)\n",
    "print(f'Amount of values in the data: {df_RA_LA.shape[0]*df_RA_LA.shape[1]}')\n",
    "print('Sum of outliers in right arm data:', outliers_RA.sum().sum())\n",
    "print('Sum of outliers in left arm data:', outliers_LA.sum().sum())\n",
    "print('Sum of outliers in both arms data:', outliers_RA_LA.sum().sum())\n",
    "print(f'Percentage of outliers in the data: {round(outliers_RA_LA.sum().sum()/(df_RA_LA.shape[0]*df_RA_LA.shape[1])*100,2)}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Outlier Observation:</h3>\n",
    "\n",
    "It's evident that the gyroscope data contains the most outliers. There are double the size of outliers in the right arm then on the left arm. The magnetometer observes the least amount of outliers. Overall, the total number of outliers are 2%  of the total dataframe. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**:\n",
    "\n",
    "\n",
    "Next, we delve into exploring the correlation among our features. Utilizing visual representations of Pearson correlation coefficients across variable pairs, we are able to determine positive, negative, or non-existent correlations. \n",
    "\n",
    "Because the columns 'subject' and 'activity' have categorical data, we have to use `OneHotEncoder` to convert the variables into a binary format. One hot encoding allows us to represent categorical data numerically in a way that preserves the information about the categories without imposing any ordinality assumptions.\n",
    "\n",
    "This can be done with the <code>OneHotEncoder</code> from the scikit-learn library. Because our data is not ordinal scaled and has no hierarchy we are using the <code>OneHotEncoder</code> for our data. [source](https://contactsunny.medium.com/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_RA_LA.copy()\n",
    "\n",
    "# Convert column 'subject' and 'activity' to one-hot encoded values\n",
    "df_corr.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Applying OneHotEncoder to transform the categorical data into binary data\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output= False).set_output(transform='pandas')\n",
    "df_ohe= ohe.fit_transform(df_corr[['subject', 'activity']])\n",
    "df_corr = pd.concat([df_corr, df_ohe], axis = 1).drop(columns=['subject', 'activity'], axis=1)\n",
    "display(df_corr.head())\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = df_corr.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))  # Source: https://stackoverflow.com/questions/2318529/plotting-only-upper-lower-triangle-of-a-heatmap\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr, annot=True, cmap='Spectral_r', linewidth=0.5, fmt='.2f', annot_kws={\"size\": 8}, mask=mask, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of the Apple Watch (without outliers)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, rotation=90)\n",
    "plt.yticks(fontsize=12, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Correlation Matrix Observation:</h3>\n",
    "\n",
    "We can observe that there is no correlation among the subjects.\n",
    "\n",
    "**Strong Positive Correlations:**\n",
    "1. **RA_ygyro and LA_zgyro:**\n",
    "   - This correlation suggests that the rotational acceleration of the right arm along the y-axis tends to increase or decrease in tandem with the rotational acceleration of the left arm along the z-axis.\n",
    "\n",
    "2. **RA_zgyro and LA_zgyro:**\n",
    "   - Indicates a strong positive relationship between the rotational acceleration of the right arm along the z-axis and that of the left arm along the z-axis, implying similar patterns of movement in both arms.\n",
    "\n",
    "3. **RA_xmag and LA_xmag:**\n",
    "   - Shows a strong positive correlation between the magnetic field intensity along the x-axis of the right arm and that of the left arm, suggesting similar magnetic field readings along their respective x-axes.\n",
    "\n",
    "4. **RA_xmag and LA_ymag:**\n",
    "   - Indicates that the magnetic field intensity along the x-axis of the right arm correlates strongly with the intensity along the y-axis of the left arm, implying some level of alignment or coordination between these axes.\n",
    "\n",
    "5. **RA_ymag and subject_p6:**\n",
    "   - Demonstrates a strong positive correlation between the magnetic field intensity along the y-axis of the right arm and the data associated with subject_p6, potentially indicating a relationship specific to that subject's activities.\n",
    "\n",
    "6. **RA_zmag and subject_p4:**\n",
    "   - Shows a strong positive correlation between the magnetic field intensity along the z-axis of the right arm and the data associated with subject_p4, suggesting a connection between the arm's movement and the activities performed by subject_p4.\n",
    "\n",
    "**Strong Negative Correlations:**\n",
    "1. **RA_xacc and LA_xmag:**\n",
    "   - Indicates a strong negative correlation between the acceleration along the x-axis of the right arm and the magnetic field intensity along the x-axis of the left arm, implying an inverse relationship between these two measurements.\n",
    "\n",
    "2. **RA_yacc and LA_yacc:**\n",
    "   - Shows a strong negative correlation between the acceleration along the y-axis of the right arm and that of the left arm, suggesting an inverse relationship between their respective accelerations along the y-axis.\n",
    "\n",
    "3. **RA_zacc and RA_zmag:**\n",
    "   - Suggests a strong negative correlation between the acceleration along the z-axis of the right arm and the magnetic field intensity along its z-axis, indicating an inverse relationship between these two measurements.\n",
    "\n",
    "4. **RA_zacc and subject_p4:**\n",
    "   - Indicates a strong negative correlation between the acceleration along the z-axis of the right arm and the data associated with subject_p4, suggesting a potential relationship between the arm's movement along the z-axis and the activities performed by subject_p4.\n",
    "\n",
    "5. **RA_ygyro and LA_ygyro:**\n",
    "   - Demonstrates a strong negative correlation between the rotational acceleration along the y-axis of the right arm and that of the left arm, implying an inverse relationship between their respective rotational accelerations along the y-axis.\n",
    "\n",
    "6. **RA_xmag and subject_p7:**\n",
    "   - Shows a strong negative correlation between the magnetic field intensity along the x-axis of the right arm and the data associated with subject_p7, indicating a potential relationship between the arm's magnetic field readings and the activities performed by subject_p7.\n",
    "\n",
    "7. **RA_ymag and subject_p5:**\n",
    "   - Indicates a strong negative correlation between the magnetic field intensity along the y-axis of the right arm and the data associated with subject_p5, suggesting a potential relationship between the arm's magnetic field readings along the y-axis and the activities performed by subject_p5.\n",
    "\n",
    "8. **LA_xmag and subject_p7:**\n",
    "   - Demonstrates a strong negative correlation between the magnetic field intensity along the x-axis of the left arm and the data associated with subject_p7, suggesting a potential relationship between the left arm's magnetic field readings and the activities performed by subject_p7.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison between two different subjects in one activity:**\n",
    "\n",
    "In this extended analysis, we aim to compare the data of two subjects engaging in the specific activity of running. We seek to observe any noticeable differences between their accelerometer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def filter_by_subject(df: pd.DataFrame, subject: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame by subject.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing data.\n",
    "        subject (str): Subject identifier.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing data for the specified subject.\n",
    "    \"\"\"\n",
    "    return df.loc[df['subject'] == subject]\n",
    "\n",
    "def plot_histogram(df1: pd.DataFrame, df2: pd.DataFrame, activity: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot histograms of accelerometer data for two subjects in a specified activity.\n",
    "\n",
    "    Parameters:\n",
    "        df1 (pd.DataFrame): DataFrame containing data for the first subject.\n",
    "        df2 (pd.DataFrame): DataFrame containing data for the second subject.\n",
    "        activity (str): Activity name for which histograms are plotted.\n",
    "    \"\"\"\n",
    "    # Filter DataFrames by activity\n",
    "    df1 = df1.loc[df1['activity'] == activity]\n",
    "    df2 = df2.loc[df2['activity'] == activity]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(16,10))\n",
    "\n",
    "    # Plot histograms for each accelerometer component\n",
    "    for i, col in enumerate(['RA_xacc', 'RA_yacc', 'RA_zacc']):\n",
    "        sns.histplot(df1[col], ax=ax[0,i], kde=True)\n",
    "        sns.histplot(df2[col], ax=ax[1,i], kde=True)\n",
    "\n",
    "        # Set titles\n",
    "        ax[0,i].set_title(f\"Histogram for Subject {df1['subject'].unique()} in activity: {activity}\", fontsize=14, fontweight='bold')\n",
    "        ax[1,i].set_title(f\"Histogram for Subject {df2['subject'].unique()} in activity: {activity}\", fontsize=14, fontweight='bold')\n",
    "\n",
    "        # Add mean and standard deviation information\n",
    "        ax[0,i].text(0.7, 0.9, f\"Mean: {df1[col].mean(): 0.2f}\\nStd: {df1[col].std(): 0.2f}\", horizontalalignment='left', verticalalignment='top', transform=ax[0,i].transAxes)\n",
    "        ax[1,i].text(0.7, 0.9, f\"Mean: {df2[col].mean(): 0.2f}\\nStd: {df2[col].std(): 0.2f}\", horizontalalignment='left', verticalalignment='top', transform=ax[1,i].transAxes)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define subjects and activities\n",
    "subjects = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "activities = [df_RA_LA['activity'].unique()[0]]\n",
    "\n",
    "# Filter DataFrames for each subject\n",
    "dfs = {subject: filter_by_subject(df_RA_LA, subject) for subject in subjects}\n",
    "\n",
    "# Plot histograms for specified activities and subjects\n",
    "for activity in activities:\n",
    "    plot_histogram(dfs['p2'], dfs['p8'], activity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Histogram Observation:</h3>\n",
    "\n",
    "- Differences in mean values along the x-axis are evident. Subject p2 exhibits a negative mean x-acceleration, while subject p8 displays a positive mean. Although the standard deviation slightly varies between p2 and p8, the difference is not substantial. Both subjects exhibit two peaks along the x-axis, with subject p2's left peak being higher, and subject p8's right peak being higher.\n",
    "  \n",
    "- Mean values along the y-axis differ between the two subjects, while standard deviations along this axis are comparable. Additionally, both subjects display two peaks along the y-axis, with the left peak being higher than the right one for both.\n",
    "  \n",
    "- There are slight variations in mean values along the z-axis. Subject p2's mean is closer to 0, while subject p8's mean is closer to 1. However, the standard deviation differs significantly between the two subjects. This indicates that values for subject p8 are more tightly clustered around its mean, whereas for subject p2, the values are more spread out.\n",
    "\n",
    "These findings suggest that individual differences in movement patterns and tendencies may influence accelerometer data, emphasizing the importance of personalized analysis in activity monitoring and assessment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate whether we can identify steps within the accelerometer data. To accomplish this, we compute the total magnitude of all accelerometer axes by summing them up. This resulting column is labeled `acc_magnitude`. Using the `find_peaks` function, we determine the number of peaks within a 60-second timeframe. Given our sampling frequency of 25Hz, this timeframe corresponds to 1500 rows in our dataset. For this we just look at the data of subject p8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "timeframe = 1500 # equal to 60s (60s / 0.04s = 1500)\n",
    "\n",
    "# defining the dataframe for just subject p8 as a example \n",
    "df_running = df_RA_LA.loc[df_RA_LA['subject'] == 'p8']\n",
    "df_running = df_running[['timestamp', 'RA_xacc', 'RA_yacc', 'RA_zacc']][:timeframe]\n",
    "df_running['acc_magnitude'] = df_running['RA_xacc'] + df_running['RA_yacc'] + df_running['RA_zacc']\n",
    "mean_magnitude = df_running['acc_magnitude'].mean()\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax = sns.lineplot(data=df_running, x='timestamp', y='acc_magnitude', ax = ax, label='acc_magnitude')\n",
    "\n",
    "# Find peaks\n",
    "peaks, _ = find_peaks(df_running['acc_magnitude'], height=mean_magnitude)\n",
    "\n",
    "# Plot peaks\n",
    "plt.plot(df_running['timestamp'].iloc[peaks], df_running['acc_magnitude'].iloc[peaks], 'ro', label='peaks' )\n",
    "plt.legend()\n",
    "\n",
    "print(f'Number of steps in {timeframe*0.04} sek are: {len(peaks)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Total Magnitude Peaks Observation:</h3>\n",
    "\n",
    "In this plot, we observe the total magnitude of the accelerometer data. Within the 60-second timeframe, we identify 161 peaks. On average, a person takes between 160 to 170 steps per minute while running, as outlined in this [source](https://www.runnersworld.de/training-basiswissen/die-richtige-schrittlaenge-und-schrittfrequenz/). This suggests the potential for detecting steps within the running activity. However, further investigation is required to explore this hypothesis in greater detail.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder - Transform Categorical values to binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise in the correlation matrix, it is necessary to transform categorical features to binary data, before we can apply the cluster algorithms. \n",
    "\n",
    "Due to the complexity of the dataset and time-consumption when running several clustering algorithms when using the entire dataset, we're selecting a representative sample comprising only 50% of the 1,140,000 rows by using `.sample(frac=0.50)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using just 10% of the for the clustering algorithms.\n",
    "df_RA_LA = df_RA_LA.copy().sample(frac=0.5, random_state=42) \n",
    "\n",
    "# Convert column 'subject' and 'activity' to one-hot encoded values\n",
    "df_RA_LA.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Applying the OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output= False).set_output(transform='pandas')\n",
    "df_ohe= ohe.fit_transform(df_RA_LA[['subject', 'activity']])\n",
    "df_tf = pd.concat([df_RA_LA, df_ohe], axis = 1).drop(columns=['subject', 'activity'], axis=1)\n",
    "display(df_tf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>OneHotEncoder Observation:</h3>\n",
    "We dropped the 'activity' and `subject` column from the dataset and created with the OneHotEncoder new columns for each `activity` and `subject`. \n",
    "Those new columns have now values of 1 and 0, depending on the subject and activity that the rows represent.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selecting the right clustering algorithm, we first take a look at the shape of the data. Because we have a highly dimensional dataset we compare each numerical feature with another numerical feature. This is done with the `.pairplot()` function. We use `.sample(frac=0.5)` to use a representative sample of 50% of the data to have a quicker runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_RA_LA.sample(frac=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Data shape Observation:</h3>\n",
    "\n",
    "We can observe from the paiplot, that we have several dense data clouds with some small separate data clouds which could be distinct clusters. We can see that the data has both, spherical shapes and complex shapes. Based on this observation we select in the next chapter the clustering algorithms. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 – Clustering\n",
    "\n",
    "For the clustering task we choose KMeans, DBSCAN and OPTICS as our initial cluster algorithms. \n",
    "\n",
    "**KMeans:**\n",
    "KMeans partitions data into k clusters by iteratively updating cluster centroids.\n",
    "KMeans is a well established Cluster Algorythm. It is simple and fast, suitable for large datasets *[Source](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages?hl=de)*. Also KMeans Algortihm is commonly used because it is good to evaluate the clusters with the Silhouette score. It works well when clusters are spherical. Because our dataset contains spherical shapes, we are choosing KMeans for the first clustering attempt.\n",
    "\n",
    "**DBSCAN:**\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups together closely packed points based on density within their neighborhood. DBSCAN automatically detects the number of clusters without requiring prior specification of the number of clusters, making it useful for datasets with unknown cluster structures. It is robust to noise and outliers since it doesn't force all points into clusters and can identify them as noise. DBSCAN can identify clusters of complex shapes and sizes, making it suitable for datasets with complex structures.\n",
    "\n",
    "**OPTICS**\n",
    "OPTICS (Ordering Points To Identify the Clustering Structure) is like DBSCAN a density-based clustering algorithm and similar to DBSCAN, but with the added capability of detecting clusters of varying density and noise more effectively. It can identify clusters of different size of density, making it suitable for datasets with potential sub-clusters and complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "\n",
    "Before applying the cluster algorithms it is important to scale the data, so that variables with lager scales or variances can not dominate the metric distance computations or different units in the data, which lead to biased results. Given the presence of numerous outliers within the dataset, which has impact on the MinMaxScaler, we use the StandardScaler for data normalization. This transformation process ensures that features are rescaled to a distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "This mathematical transformation is expressed as:\n",
    "\n",
    "$z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "With:\n",
    "- $x$ represents the original feature vector,\n",
    "- $\\mu$ denotes the mean of the feature vector,\n",
    "- $\\sigma$ signifies the standard deviation of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_tf)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_tf.columns)\n",
    "df_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Data Scaling Observation:</h3>\n",
    "\n",
    "We've now standardized our data, setting the mean to 0 and ensuring that each column has a standard deviation of 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "Initially, our objective is to employ the KMeans Algorithm to identify potential clusters within the data. We use the Elbow method to determine an optimal value for $k$. \n",
    "\n",
    "To calculate the inertia for KMeans, we have to compute the sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "Here is a short introduction how KMeans works:\n",
    "1. Randomly select K points from your dataset as the initial cluster centers (centroids).\n",
    "  \n",
    "2. Assign each data point to the nearest centroid, creating K clusters.\n",
    "  \n",
    "3. Recalculate the centroids of the clusters by taking the mean of all data points assigned to each cluster.\n",
    "  \n",
    "4. Repeat steps 2 and 3 until convergence. Convergence occurs when the centroids do not change significantly between iterations, or after a fixed number of iterations.\n",
    "\n",
    "5. **Calculate Inertia**: After convergence, calculate the inertia, which is the sum of squared distances of samples to their closest cluster center. It's a measure of how internally coherent the clusters are. Mathematically, it's given by:\n",
    "\n",
    "$\\text{Inertia} = \\sum_{i=0}^{n} \\min_{j=1}^{k} \\left( ||x_i - \\mu_j||^2 \\right)$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of samples.\n",
    "- $k$ is the number of clusters.\n",
    "- $x_i$ is the i-th data point.\n",
    "- $mu_j$is the centroid of the cluster to which \\( x_i \\) is assigned.\n",
    "- $|| \\cdot ||^2$ denotes the squared Euclidean distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_sc = KMeans(random_state=42, n_init=10)\n",
    "# print(k_values,silhouette_values)\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "visualizer = KElbowVisualizer(kmeans_sc, k=(2, 20))\n",
    "visualizer.fit(df_scaled)        # Fit the data to the visualizer\n",
    "k_value_KMeans = visualizer.elbow_value_\n",
    "\n",
    "print(f'Number of Clusers for KMeans = {k_value_KMeans}')\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>KMeans Grid Search and Elbow Method Observation:</h3>\n",
    "\n",
    "The outcome of the Elbow method to determine the optimal value for $k$ indicates that $k$ equals 8, but still having a relatively high inertia.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take a look at the silhouette score. The silhouette score measures the similarity of an object to its own cluster relative to other clusters, with a scale spanning from -1 to 1. Higher scores indicate strong correspondence to the assigned cluster and weak alignment with neighboring clusters. The silhouette plot is used to visualize and interpret the silhouette scores of individual data points within clusters. \n",
    "\n",
    "The scores can be interpreted according to [Source](https://de.wikipedia.org/wiki/Silhouettenkoeffizient):\n",
    "| Structuring     | Value Range of S(o)         |\n",
    "|-----------------|-----------------------------|\n",
    "| strong          | 0.75 < S(o) ≤ 1             |\n",
    "| moderate        | 0.5 < S(o) ≤ 0.75           |\n",
    "| weak            | 0.25 < S(o) ≤ 0.5           |\n",
    "| no structure    | 0 < S(o) ≤ 0.25             |\n",
    "\n",
    "The formula for the silhouette score for a single sample is:\n",
    "\n",
    "$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$\n",
    "\n",
    "Where:\n",
    "- $s(i)$ is the silhouette score for the i-th sample.\n",
    "- $a(i)$ is the average distance from the i-th sample to other samples in the same cluster.\n",
    "- $b(i)$ is the smallest average distance from the i-th sample to samples in a different cluster, minimized over clusters.\n",
    "\n",
    "It provides a graphical representation of how well each data point fits into its assigned cluster. This plot helps in assessing the quality of clustering by revealing the compactness and separation of clusters. A well-defined silhouette plot with predominantly high scores indicates a good clustering solution, whereas a plot with mixed or low scores suggests potential issues with clustering quality, such as overlapping clusters or poorly defined boundaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random color based on k_clusters (Source: https://datascientyst.com/get-list-of-n-different-colors-names-python-pandas/#:~:text=How%20to%20Get%20a%20List%20of%20N%20Different,values%20format%20%28HTML%20and%20CSS%29%20in%20Python%20)\n",
    "n = k_value_KMeans\n",
    "colors = ['#%06X' % randint(0, 0xFFFFFF) for i in range(n)]\n",
    "\n",
    "# Applying KMeans\n",
    "kmeans_sc = KMeans(n_clusters=k_value_KMeans, random_state=42, n_init=10)\n",
    "cluster_assignments = kmeans_sc.fit_predict(df_scaled)\n",
    "\n",
    "# Applying Sihlouette Visualization\n",
    "visualizer = SilhouetteVisualizer(kmeans_sc, colors=colors)\n",
    "visualizer.fit(df_scaled)        \n",
    "visualizer.finalize()\n",
    "print(f'Silhoutte score: {visualizer.silhouette_score_}')\n",
    "visualizer.show();        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Silhouette Score Observation:</h3>\n",
    "\n",
    "- A silhouette score of 0.32 suggests that the clusters are weak-structure, with some overlap between neighboring clusters. \n",
    "- 8 clusters have been created of different size\n",
    "- some clusters have reasonably well-separated silhouettes\n",
    "- some clusters have a very low silhouettes score and some values are negative suggesting presence of points that are closer to members of another cluster than to the one they have been assigned to.\n",
    "\n",
    "\n",
    "**Conclusion:** KMeans does not give a good result for this dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never the less, we take a look at the KMeans clusters although we got a bad silhouette score. We can see the clusters in the sensor data with a ``pairplot``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_assignments(df, model, pca=False, sample_frac=0.1, height=2.5, alpha=0.8)->None:\n",
    "    \"\"\"\n",
    "    Plot cluster assignments of the data using pairplot.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe containing the data.\n",
    "        model: The clustering model used for assigning clusters.\n",
    "        pca (bool, optional): Indicates whether the data is from PCA. Defaults to False.\n",
    "        sample_frac (float, optional): The fraction of samples to be used for plotting. Defaults to 0.1.\n",
    "        height (float, optional): The height of the pairplot. Defaults to 2.5.\n",
    "        alpha (float, optional): The transparency of data points in the plot. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "        \n",
    "    df['cluster_assignments'] = model.labels_\n",
    "\n",
    "    if pca != True:\n",
    "        selected_features = [\n",
    "            'RA_xacc', 'RA_yacc', 'RA_zacc', \n",
    "            'RA_xgyro', 'RA_ygyro', 'RA_zgyro',\n",
    "            'RA_xmag', 'RA_ymag', 'RA_zmag',\n",
    "            'cluster_assignments']\n",
    "    else:\n",
    "        selected_features = df.columns # For PCA data\n",
    "\n",
    "    # Create a custom color palette\n",
    "    palette = sns.color_palette('hls', df['cluster_assignments'].nunique())\n",
    "    \n",
    "    # Create a mapping from cluster assignments to colors\n",
    "    cluster_colors = {label: color for label, color in zip(df['cluster_assignments'].unique(), palette)}\n",
    "    \n",
    "    # Assign a color to -1 (noise points)\n",
    "    cluster_colors[-1] = (0, 0, 0)\n",
    "    \n",
    "    sns.pairplot(df[selected_features].sample(frac=sample_frac, random_state=42), hue='cluster_assignments', palette=cluster_colors, height=height, plot_kws={'alpha': alpha})\n",
    "    plt.show()\n",
    "    \n",
    "    df.drop(['cluster_assignments'], inplace=True, axis=1)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_assignments(df_scaled, kmeans_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>KMeans Cluster Observation</h3>\n",
    " \n",
    "Observing the pairplot, it appears that we've effectively partitioned the data into nine observable clusters. Any points that seemed indicative of noise were allocated to a cluster.\n",
    "\n",
    "We aim to identify and address such noise clusters using alternative algorithms. \n",
    "\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN/OPTICS Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN -applied to scaled data\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm for clusters with irregular shapes and densities. \n",
    "To find the best hyperparameter $\\epsilon$ and $MinPts$ for DBSCAN, we first calculating the distance for each point among its neigbors. This will give us the elbow curve to find density of the data points and their minimum distance $\\epsilon$ values. Because $MinPts$ should be selected bigger then the number of columns in the dataset, we select double the size of number of columns for $MinPts$ according to this *[Source](https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd)*.\n",
    "\n",
    "\n",
    "\n",
    "DBSCAN works in the following steps:\n",
    "1. If there are at least MinPts points within the $\\varepsilon$ neighborhood of the chosen point (including the point itself), mark this point as a core point. Then, recursively find and mark all points that are directly or indirectly density-reachable from this core point.\n",
    "2. Repeat the above steps for all unvisited points until all points have been visited.\n",
    "3. Any points that were not visited during the process and are not within the $\\varepsilon$ neighborhood of any core point are considered noise and typically not assigned to any cluster.\n",
    "\n",
    "DBSCAN has several advantages:\n",
    "\n",
    "- It does not require the number of clusters to be specified in advance.\n",
    "- It can find arbitrarily shaped clusters and is robust to outliers.\n",
    "- It's relatively efficient, especially for large datasets, as it only needs to compute pairwise distances within the epsilon neighborhood.\n",
    "\n",
    "However, DBSCAN also has some drawbacks:\n",
    "\n",
    "- It may struggle with clusters of varying densities.\n",
    "- It's sensitive to the choice of parameters $\\varepsilon$ and MinPts, which can impact the resulting clusters.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(df: pd.DataFrame, metric: str)->float:\n",
    "    '''  \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        metric (str): The distance metric to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The optimal epsilon value.\n",
    "    '''\n",
    "    \n",
    "    # Fit the nearest neighbors estimator\n",
    "    nn = NearestNeighbors(n_neighbors=2*df.shape[1], metric=metric)\n",
    "    nbrs = nn.fit(df)\n",
    "    distances, indices = nbrs.kneighbors(df)\n",
    "\n",
    "    # Sort and plot distances\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "\n",
    "    # Finding the knee point: Source: https://kneed.readthedocs.io/en/stable/parameters.html\n",
    "    kneedle = KneeLocator( np.arange(len(distances)) , distances , S=1.0, curve=\"convex\", direction=\"increasing\")\n",
    "    \n",
    "    plt.plot(distances)\n",
    "    plt.axhline(y=kneedle.knee_y, color='r', linestyle='--')\n",
    "    plt.xlabel('Points (sample) sorted by distance')\n",
    "    plt.ylabel(f'{2*df.shape[1]}-NN Distance')\n",
    "    plt.title(f'Elbow Plot for optimal eps with {metric} metric')\n",
    "    plt.show()\n",
    "\n",
    "    return kneedle.knee_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Elbow Plot Observation</h3>\n",
    " \n",
    " Calculating the knee point with the ``KneeLocator`` suggests to use the ideal $\\epsilon$ value of 2.79.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $MinPts$ we use values bigger then our number of features. According to *[Source](https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd)* we select the double features size for $MinPts$.\n",
    "\n",
    "For choosing the right distance metric we can simply iterate over the different metrics and compare the ouput clusters. \n",
    "\n",
    "For choosing the right metric, we can already exclude:\n",
    "\n",
    "Hamming-Distance: Running the Hamming-distance on this dataset gave us just noise data points. Although column 'subject' is categorical feature, but because we have just one categorical data and nine numerical features are not selecting this metric as our choice. Further we cannot combine the Hamming-Distance with e.g. euclidean which would be the most suitable for this kind of data like the Gower-Distance, which is not implemented in the scikit-learn libary DBSCAN/OPTICS yet.\n",
    "\n",
    "Jacaard-Distance: Is used for similarity of two sets e.g. if two persons have the same friend in facebook.\n",
    "\n",
    "Cosine-Metric: Is a similarity function which ignores the distance and just looks a the orientation. It is not a proper metric (triangle inequality).\n",
    "\n",
    "Levenshtein-Distance: Is a distance between two strings and measure the number of edit operations for tuning one string into the other e.g search engine. Our dataset does not contain such string data.\n",
    "\n",
    "Thus, we are selecting between the manhattan and the euclidean distance for this dataset.\n",
    "Next make a cluster comparison between those two metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between Manhattan and Euclidean Distance\n",
    "print('Comparison between Distances: \\n')\n",
    "for metric in ['manhattan', 'euclidean']:\n",
    "    print(f'{metric}: ')\n",
    "    eps = plot_elbow(df_scaled, metric)\n",
    "    print(f'Knee Point for {metric} = {round(eps, 2)}')\n",
    "    dbscan_test = DBSCAN(eps=eps, min_samples=2*df_scaled.shape[1], metric=metric, n_jobs=1)\n",
    "    dbscan_clusters_test = dbscan_test.fit_predict(df_scaled) \n",
    "    print(f'Number of Cluster: {len(np.unique(dbscan_clusters_test)[1:])} \\n  Metric: {metric} \\n Cluster: {np.unique(dbscan_clusters_test)[:]} \\n \\n')\n",
    "    #plot_cluster_assignments(df=df_scaled, model=dbscan_test)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Choosing Metric Observation:</h3>\n",
    "Comparing the Manhattan and the Euclidean Distance, we get different cluster sizes.\n",
    "\n",
    "Hint: Because the cluster results with the manhattan-distance varies drastically in the following chapters OPTICS with PCA and without, we come to the conclusion that the euclidean distance in more suitable for this experiment, because it gives consistent clusters.\n",
    "\n",
    "For the following cluster analysis we select the euclidean-distance as our chosen metric.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'euclidean'\n",
    "eps = plot_elbow(df_scaled, metric)\n",
    "dbscan_sc=DBSCAN(eps=eps, min_samples=2*df_scaled.shape[1], metric=metric, n_jobs=1)\n",
    "dbscan_clusters_sc = dbscan_sc.fit_predict(df_scaled) \n",
    "print(f'Number of Cluster: {len(np.unique(dbscan_clusters_sc)[1:])} \\n  Cluster: {np.unique(dbscan_clusters_sc)[:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_assignments(df=df_scaled, model=dbscan_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>DBSCAN Clustering Observation:</h3>\n",
    "\n",
    "With the Optics we are now able to detect the noise in the data, which can be seen in the black dots, also labeled as -1 in the pairplot. We were able to detect 9 clusters which is one cluster more then with KMeans. Looking at the pairplot we can assume that the clustering was successful. Unfortunately there is no measurement method to determine the quality of the cluster.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optics\n",
    "OPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm like DBSCAN, yet it offers the advantage of extracting clusters with varying densities and shapes. Particularly beneficial for observeing clusters of dissimilar densities within extensive, high-dimensional datasets.\n",
    "\n",
    "\n",
    "\n",
    "For each point, the core distance is defined as the distance to its k-th nearest neighbor. This distance gives an idea of how densely packed the neighborhood around the point is.\n",
    "If a point has a lot of close neighbors, its core distance is small. If the neighbors are far away, its core distance is large.\n",
    "\n",
    "The reachability distance of a point `A` with respect to another point `B` is calculated as the maximum of `B`'s core distance and the actual distance between `A` and `B`.\n",
    "This measure helps determine how \"reachable\" a point is from another point, taking into account the density around the starting point.\n",
    "\n",
    "In the reachability plot, valleys (low reachability distances) indicate dense regions or clusters, while peaks (high reachability distances) indicate the boundaries between clusters.\n",
    "By identifying these valleys and peaks, we can extract clusters from the reachability plot.\n",
    "\n",
    "Like for DBSCAN we use the same parameters for $MinPts$ = $MinSamples$. For $MinClusterSize$ we use the rule of thumb and select the amount of features within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optics - with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reachability(model)->None:\n",
    "    \"\"\"\n",
    "    Plot the reachability plot of an OPTICS clustering model.\n",
    "\n",
    "    Parameters:\n",
    "        model (OPTICS): Fitted OPTICS clustering model.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Reachability Plot\n",
    "    reachability = model.reachability_[model.ordering_]\n",
    "    labels = model.labels_[model.ordering_]\n",
    "    colors = sns.color_palette('husl', len(set(model.labels_)))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i, klass in enumerate(np.unique(labels)):\n",
    "        if klass == -1: # Set noise to color black\n",
    "            color = 'k'\n",
    "            label = 'Noise'\n",
    "        else:\n",
    "            color = colors[i]\n",
    "            label = f'Cluster {klass}'\n",
    "            i+=1\n",
    "        mask = labels == klass\n",
    "        plt.scatter(np.arange(len(reachability))[mask], reachability[mask], marker='o', color=color, label=label, s=12)\n",
    "\n",
    "    plt.title('OPTICS Reachability Plot', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Ordered Points')\n",
    "    plt.ylabel('Reachability')\n",
    "\n",
    "    # Place the legend outside of the figure\n",
    "    plt.legend(fontsize=12, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "# Applying Optics\n",
    "optics_sc = OPTICS(min_samples=2*df_scaled.shape[1], xi=0.05, min_cluster_size=df_scaled.shape[1], metric=metric)\n",
    "optics_sc.fit(df_scaled) \n",
    "\n",
    "labels = optics_sc.labels_\n",
    "\n",
    "# Number of Cluster (without noise)\n",
    "n_clusters = len(set(labels))\n",
    "print(f'Number of Cluster (with noise): {n_clusters}')\n",
    "print(set(labels))\n",
    "\n",
    "plot_reachability(model=optics_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>OPTICS Clustering Observation:</h3>\n",
    "\n",
    "Applying OPTICS gave us 4 clusters and noise (KMeans = 8, DBSCAN = 8). \n",
    "\n",
    "**The Reachability Plot gives us following information:**\n",
    "- There are clusters where the data points with a very low reachability distance, indicating high density Clusters. These regions correspond to dense clusters in the dataset.\n",
    "- Valleys in the plot represent areas of lower density between clusters. Valleys separate distinct clusters and can help in determining the optimal clustering structure. It can assumed that there are more clusters, because some valleys were determine as noise and two valleys were determine as one cluster.\n",
    "- On the other hand we can see that there a multiple valleys where labeled as noise but the points still have a low distance to each other and could form another cluster.\n",
    "- Other points are very far away from each other but still are labeled as clusters. Other points have a very large distance and clearly are labeled as noise. Smaller reachability distances indicate denser regions, while larger distances represent sparser areas or outliers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_assignments(df=df_scaled, model=optics_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>OPTICS Clustering Observation:</h3>\n",
    "\n",
    "\n",
    "**Clustering Pairplot:**\n",
    "\n",
    "The clusters in the paiplot are looking reasonable. \n",
    "We can see that the data points form a reasonable cluster. Comparing the Clusters with KMeans, we can see now clusters with a different shapes. \n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compare the results of OPTICS and DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "fig, ax1 = plt.subplots(figsize=(6,6))\n",
    "label_font = {'size':'12'}  # Adjust to fit\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(optics_sc.labels_, dbscan_sc.labels_ , ax=ax1, colorbar=False)\n",
    "fig.suptitle('Cluster Comparison OPTICS vs. DBSCAN', fontsize = 20)\n",
    "ax1.set_xlabel('DBSCAN', fontdict=label_font);\n",
    "ax1.set_ylabel('OPTICS', fontdict=label_font);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Cluster Comparison OPTICS vs. DBSCAN</h3>\n",
    "\n",
    "- While a couple of data points were labeled as noise in Optics, they were labeled as clusters in DBSCAN.\n",
    "- The overall cluster size of DBSCAN is very similar.\n",
    "- A very small number of data points were labeled as a cluster by OPTICS but were labeled as noise by DBSCAN.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_RA_LA.copy()\n",
    "df_results['clusters_dbscan_sc'] = dbscan_sc.labels_\n",
    "df_results['clusters_optics_sc'] = optics_sc.labels_\n",
    "print(sorted(df_results['clusters_dbscan_sc'].unique()))\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'subject' column\n",
    "df_results['subject_numeric'] = le.fit_transform(df_results['subject'])\n",
    "\n",
    "# Create a cross-tabulation of 'subject_numeric' and 'clusters'\n",
    "cross_tab = pd.crosstab(df_results['clusters_dbscan_sc'], df_results['subject_numeric'])\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(cross_tab, annot=True, cmap='Spectral_r', fmt='d', ax=ax)\n",
    "fig.suptitle('Subject Cluster assignment of DBSCAN', fontsize = 20)\n",
    "ax.set_xticklabels(['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Subject Cluster assignment of DBSCAN Observation</h3>\n",
    "\n",
    "- the clusters are bonded on the subjects, basically we were clustering the subjects\n",
    "- we get similar size of clusters \n",
    "- every person is obtained in one cluster or noise\n",
    "- No subject share the same cluster with another subject\n",
    "- P1, P2, P3, P4 and P8 have data points in noise, but P2 stands out with the most points in noise.\n",
    "\n",
    "  \n",
    "This implies that there aren't clear patterns or differences among the subjects.\n",
    "\n",
    "**Thus, because we were able to determine sub-cluster with DBSCAN we are selecting this algorithm as our choice.**\n",
    "\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 – Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a method that rotates the dataset in a way such that the rotated features are statistically uncorrelated. This rotation is often followed by selecting only a subset of the new features, according to how important they are for explaining the data. It helps in simplifying complex datasets by reducing the number of variables while preserving the most important information. This transformation effectively reduces the dimensionality of the dataset while retaining as much of the original variance as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = PCA()\n",
    "pca_transformed = PCA.fit_transform(df_scaled)\n",
    "df_pca = pd.DataFrame(pca_transformed, index=df_scaled.index)\n",
    "df_pca.cov().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>PCA transformation Observation</h3>\n",
    "\n",
    "The transformed data shows no covariance (zero values) among different components and the components are ordered by their variance (diagonal values) (highest to lowest, see main diagonal)\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has not been reduced. To reduce the data, we take a look at the accumulated percentage of the explained variance of the pca-transformed data and select just the 80% of the explained variances within the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = pca(n_components=0.80)\n",
    "\n",
    "# Fit transform\n",
    "results = model_pca.fit_transform(df_pca)\n",
    "# explained variance\n",
    "fig, ax = model_pca.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Accumulated Explained Variance Observation:</h3>\n",
    "\n",
    "- 10 of 27 components  explain more 80% of the variance. \n",
    "- 6 of 27 components explain already more then 60% of the variance. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = model_pca.results['PC']\n",
    "df_pca.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>PCA Dataset Observation:</h3>\n",
    "\n",
    "We are using just 80% of the explained variance columns. Taht means that with PCA we decreased the size of columns from 27 to 10. That can lead to a significant faster runtime because we are just using less then half of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optics with PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_pca = OPTICS(min_samples=2*df_pca.shape[1], xi=0.05, min_cluster_size=df_scaled.shape[1], metric=metric)\n",
    "optics_pca.fit(df_pca) \n",
    "\n",
    "labels = optics_pca.labels_\n",
    "\n",
    "# Number of Cluster (without noise)\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f'Number of Optics Clusters: {n_clusters_}')\n",
    "\n",
    "plot_reachability(optics_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>OPTICS with PCA Observation: </h3>\n",
    " \n",
    "**The Reachability Plot gives us following information:**\n",
    "\n",
    "- With PCA, we get now more dense cluster structures then without PCA.\n",
    "- Overall the density of the valleys are now more similar as before without PCA.\n",
    "\n",
    "- Certain regions exhibit clusters where data points maintain very low reachability distances, denoting areas of high density within the dataset. These dense clusters correspond to the concentrated groupings of points.\n",
    "  \n",
    "- Multiple valleys have been misclassified as noise despite points within them having low distances from one another, suggesting the potential formation of additional clusters.\n",
    "\n",
    "- Certain points are situated far apart from each other yet remain labeled as clusters. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_assignments(df=df_pca, model=optics_pca, pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>OPTICS with PCA Observation: </h3>\n",
    " \n",
    "\n",
    "**Clustering Pairplot**\n",
    "- With OPTICS with PCA we were able to detect 8 clusters (without PCA: KMeans = 8, DBSCAN = 8, OPTICS = 4). \n",
    "- Overall we have now 8 clusters which is 4 clusters more then without PCA & Optics.  \n",
    "- Through PCA, each clusters stands out more then without PCA. It also seems like, that there are clusters determine as noise eg. PC1/PC10, or PC8/PC10 and PC9/PC10 which we have also seen in the reachability plot.\n",
    "\n",
    "We can conclude that we get better results with PCA, then without.\n",
    "\n",
    "\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reachability plot of the Optics Algorithm with PCA we can observe that an $\\epsilon$ at 1.3 could assign those dense noise points to clusters. Thats why we select DBSCAN with eps=1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DBSCAN wit eps= 1.3\n",
    "eps = plot_elbow(df_pca, metric)\n",
    "print(f'Knee Point for {metric} = {round(eps, 2)}')\n",
    "dbscan_pca=DBSCAN(eps=1.3, min_samples=2*df_pca.shape[1], metric=metric, n_jobs=1)\n",
    "dbscan_clusters_pca = dbscan_pca.fit_predict(df_pca) \n",
    "print(f'Number of Cluster: {len(np.unique(dbscan_clusters_pca)[1:])} \\n  Cluster: {np.unique(dbscan_clusters_pca)[:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>DBSCAN with eps=1.3 Observation: </h3>\n",
    "\n",
    "\n",
    "- As we thought so, with eps = 1.3 we are getting now 8 clusters and noise. Which is the same cluster as we had without PCA.\n",
    "- Comparing our choice of eps with the knee plot (1.34) gives us the nearly the same result.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_assignments(df=df_pca, model=dbscan_pca, pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Pairplot with eps=1.3 Observation: </h3>\n",
    " \n",
    "With the eps = 1.3 we can see now less noise regions and that those noise points before are now assigned to clusters. \n",
    "We get better results using just 80% of the principal components. PCA is a huge improvement by reducing the features but keeping also the information within.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 - Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "fig, ax1 = plt.subplots(figsize=(6, 6))\n",
    "label_font = {'size':'12'}  # Adjust to fit\n",
    "ConfusionMatrixDisplay.from_predictions(optics_pca.labels_, dbscan_pca.labels_ , ax=ax1, colorbar=False)\n",
    "ax1.set_xlabel('DBSCAN w. PCA', fontdict=label_font);\n",
    "ax1.set_ylabel('OPTICS w. PCA', fontdict=label_font);\n",
    "\n",
    "#optics_pca.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Confusion Matrix Observation: </h3>\n",
    "\n",
    "**Comparing OPTICS with PCA vs. DBSCAN with PCA:**\n",
    "- We can see now how many data points were labeled as noise before we were setting the eps to 1.3 which then were labeled to clusters.\n",
    "- We can almost similar size of clusters\n",
    "- Five Clusters have a similar cluster size. \n",
    "- Some clusters have a very small amount of data points e.g. DBSCAN Cluster: 6 \n",
    "- As we expected data points which, where in OPTICS noise are now labeled as clusters. \n",
    "- Some data points which were separate clusters in OPTICS are now in one single cluster in DBSCAN\n",
    "\n",
    "**Because we were able to determine sub-cluster with DBSCAN we are selecting this algorithm as our choice.**\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we assign the cluster labels to our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['clusters_dbscan_pca'] = dbscan_pca.labels_\n",
    "df_results['clusters_optics_pca'] = optics_pca.labels_\n",
    "\n",
    "df_results.sort_values(by='subject', inplace=True)\n",
    "print(sorted(df_results['clusters_dbscan_pca'].unique()))\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of 'subject_numeric' and 'clusters'\n",
    "cross_tab = pd.crosstab(df_results['clusters_dbscan_pca'], df_results['subject_numeric'])\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(cross_tab, annot=True, cmap='Spectral_r', fmt='d', ax=ax)\n",
    "ax.set_xticklabels(['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    " <h3>Subject - Cluster assignment Observation: </h3>\n",
    "\n",
    "*Applying PCA to the data gives us the same cluster observation as without PCA!*\n",
    "- the clusters are bonded on the subjects, basically we were clustering the subjects\n",
    "- we get similar size of clusters \n",
    "- every person is obtained in at least one cluster\n",
    "- No subject share the same cluster with another subject\n",
    "- P1, P2, P3, P4 and P8 have data points in noise, but P2 stands out with the most points in noise.\n",
    "\n",
    "  \n",
    "This implies that there aren't clear patterns or differences among the subjects.\n",
    "No matter if we were able to get meaningful clusters, with PCA we were able to select just 80% of the features and get the same result as without. This is a huge advantage and leads us to select DBSCAN with PCA as our best choice.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9 - Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary and Conclusion**\n",
    "\n",
    "Initially, we examined several statistical properties of the data, identifying differences in statistical properties between two subjects. Subsequently, we made the assumption that we could discern step recognition within the dataset.\n",
    "\n",
    "Our clustering analysis began with an evaluation of the KMeans algorithm, revealing a silhouette score of 0.322. This indicated weak clustering across eight clusters, with some overlap observed. DBSCAN yielded a similar cluster size, but effectively excluded noise data points. Transitioning to OPTICS, we identified four distinct clusters, with visible sub-clusters evident in the reachability plot. A comparison between DBSCAN and OPTICS revealed that DBSCAN successfully labeled these sub-clusters, leading us to select DBSCAN as our preferred clustering algorithm. However, visualizing the clusters among subjects revealed that they merely corresponded to the eight subjects without meaningful distinctions.\n",
    "\n",
    "Applying PCA reduced the principal components by 80%, enhancing cluster density and separation when combined with OPTICS. Analysis of the reachability plot suggested an optimal epsilon value of 1.3 for improved clustering. Comparing this epsilon value with the knee plot for DBSCAN reaffirmed our choice. Ultimately, comparing the clustering results of OPTICS with PCA and DBSCAN with PCA led us to select DBSCAN due to its ability to detect sub-clusters. Visualization of the clusters among subjects confirmed eight clusters, each corresponding to a subject.\n",
    "\n",
    "In conclusion, the experiment failed to yield meaningful clusters, indicating the need for further steps.\n",
    "\n",
    "**Lessons Learned:**\n",
    "\n",
    "- Detected clusters exhibit complex shapes.\n",
    "- High-dimensional data can lead to runtime issues.\n",
    "- Clustering does not always produce meaningful results.\n",
    "- Clustering results may align with categorical features provided.\n",
    "- Subgroups within subjects were not identified.\n",
    "- Selecting the appropriate distance metric is challenging with mixed data types.\n",
    "- OPTICS, along with the reachability plot, aids in epsilon estimation.\n",
    "- PCA enhances clustering efficiency.\n",
    "- A representative subject size is crucial.\n",
    "\n",
    "**Future Work:**\n",
    "\n",
    "Future steps include exploring alternative clustering algorithms such as Clarans/PAM or HAC, analyzing different metrics, and considering sampling variations. Expanding sensor placements, activities, and feature engineering could lead to more meaningful results. Additionally, involving more subjects with varying fitness levels is essential.\n",
    "\n",
    "Moving forward, research in wearable technology necessitates broader exploration, encompassing more subjects, activities, and advanced feature engineering to refine data analysis. Despite the experiment's limitations, it provided valuable insights into our data and subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".05_MMS",
   "language": "python",
   "name": ".05_mms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
